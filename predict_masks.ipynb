{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict_masks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "KYHRcp62BLqS",
        "colab_type": "code",
        "outputId": "2ae9dc0b-e326-4dfc-e05f-361d0a95e166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.16.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.9.134)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.0.1.post2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2018.1.10)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.134 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.12.134)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.2.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.2)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.134->boto3->pytorch_pretrained_bert) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.134->boto3->pytorch_pretrained_bert) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.134->boto3->pytorch_pretrained_bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Ih854I-CJcz",
        "colab_type": "code",
        "outputId": "7036d7a1-813b-4242-a2a6-45b369a10708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Try to predict a single masked-out word\"\"\"\n",
        "\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "\n",
        "# Tokenized input\n",
        "text1 = \"[CLS] And the riot squad they're restless, they need somewhere to go [SEP]\"\n",
        "text2 = \"As Lady and I look out tonight, from Desolation Row. [SEP]\"\n",
        "tokenized_text1 = tokenizer.tokenize(text1)\n",
        "tokenized_text2 = tokenizer.tokenize(text2)\n",
        "tokenized_text = tokenized_text1 + tokenized_text2\n",
        "\n",
        "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
        "masked_index = tokenized_text2.index('tonight') + len(tokenized_text1)\n",
        "tokenized_text[masked_index] = '[MASK]'\n",
        "\n",
        "# Convert token to vocabulary indices\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "segments_ids = [0]*len(tokenized_text1) + [1]*(len(tokenized_text2))\n",
        "\n",
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "if torch.cuda.is_available():\n",
        "    tokens_tensor = tokens_tensor.to('cuda')\n",
        "    segments_tensors = segments_tensors.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    predictions = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "# Confirm we were able to predict the correct '[MASK]'\n",
        "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
        "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "print(\"\\n\",\"Predicted [MASK] = \",predicted_token,\"\\n\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 2679312.30B/s]\n",
            "100%|██████████| 1248501532/1248501532 [00:21<00:00, 58031939.29B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Predicted [MASK] =  together \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ja_iSf1TKhKK",
        "colab_type": "code",
        "outputId": "35f58b4e-a520-438e-a95e-28218da8e624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Try to generate from BERT \"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "MASK = \"[MASK]\"\n",
        "MASK_ATOM = \"[MASK]\"\n",
        "\n",
        "def preprocess(tokens, tokenizer):\n",
        "    \"\"\" Preprocess the sentence by tokenizing and converting to tensor \"\"\"\n",
        "    tok_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    tok_tensor = torch.tensor([tok_ids])\n",
        "    return tok_tensor\n",
        "\n",
        "\n",
        "def get_mask_ids(masking):\n",
        "    if masking:\n",
        "      mask_ids = [int(d) for d in masking.split(',')]\n",
        "    else:\n",
        "      mask_ids = []     \n",
        "    return mask_ids\n",
        "\n",
        "  \n",
        "def get_seed_sent(seed_sentence, tokenizer, masking=None, n_append_mask=0):\n",
        "    \"\"\" Get initial sentence to decode from, possible with masks \"\"\"\n",
        "\n",
        "    # Get initial mask\n",
        "    mask_ids = get_mask_ids(masking)\n",
        "\n",
        "    # Tokenize, respecting [MASK]\n",
        "    seed_sentence = seed_sentence.replace(MASK, MASK_ATOM)\n",
        "    toks = tokenizer.tokenize(seed_sentence)\n",
        "    for i, tok in enumerate(toks):\n",
        "        if tok == MASK_ATOM:\n",
        "            mask_ids.append(i)\n",
        "\n",
        "    # Mask the input\n",
        "    for mask_id in mask_ids:\n",
        "        toks[mask_id] = MASK\n",
        "\n",
        "    # Append MASKs\n",
        "    for _ in range(n_append_mask):\n",
        "        mask_ids.append(len(toks))\n",
        "        toks.append(MASK)\n",
        "    mask_ids = sorted(list(set(mask_ids)))\n",
        "\n",
        "    seg = [0] * len(toks)\n",
        "    seg_tensor = torch.tensor([seg])\n",
        "    return toks, seg_tensor, mask_ids\n",
        "\n",
        "  \n",
        "def load_model(version):\n",
        "    \"\"\" Load model \"\"\"\n",
        "    model = BertForMaskedLM.from_pretrained(version)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict(model, tokenizer, tok_tensor, seg_tensor, how_select=\"argmax\"):\n",
        "    \"\"\" Get model predictions and convert back to tokens \"\"\"\n",
        "    preds = model(tok_tensor, seg_tensor)\n",
        "\n",
        "    if how_select == \"sample\":\n",
        "        dist = Categorical(logits=F.log_softmax(preds[0], dim=-1))\n",
        "        pred_idxs = dist.sample().tolist()\n",
        "    elif how_select == \"topk\":\n",
        "        kth_vals, kth_idx = F.log_softmax(preds[0], dim=-1).topk(3, dim=-1)\n",
        "        dist = Categorical(logits=kth_vals)\n",
        "        pred_idxs = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1).tolist()\n",
        "    elif how_select == \"argmax\":\n",
        "        pred_idxs = preds.argmax(dim=-1).tolist()[0]\n",
        "    else:\n",
        "        raise NotImplementedError(\"Prediction procedure %s not found!\" % how_select)\n",
        "\n",
        "    pred_toks = tokenizer.convert_ids_to_tokens(pred_idxs)\n",
        "    return pred_toks\n",
        "\n",
        "def sequential_decoding(toks, seg_tensor, model, tokenizer, selection_strategy):\n",
        "    \"\"\" Decode from model one token at a time \"\"\"\n",
        "    for step_n in range(len(toks)):\n",
        "        tok_tensor = preprocess(toks, tokenizer)\n",
        "        pred_toks = predict(model, tokenizer, tok_tensor, seg_tensor, selection_strategy)\n",
        "        toks[step_n] = pred_toks[step_n]\n",
        "    return toks\n",
        "\n",
        "def masked_decoding(toks, seg_tensor, masks, model, tokenizer, selection_strategy):\n",
        "    \"\"\" Decode from model by replacing masks \"\"\"\n",
        "    for step_n, mask_id in enumerate(masks):\n",
        "        tok_tensor = preprocess(toks, tokenizer)\n",
        "        pred_toks = predict(model, tokenizer, tok_tensor, seg_tensor, selection_strategy)\n",
        "        toks[mask_id] = pred_toks[mask_id]\n",
        "    return toks\n",
        "\n",
        "  \n",
        "def detokenize(pred_toks):\n",
        "    \"\"\" Return the detokenized lyric prediction \"\"\"\n",
        "    new_sent = []\n",
        "    for i, tok in enumerate(pred_toks):\n",
        "        if tok.startswith(\"##\"):\n",
        "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
        "        else:\n",
        "            new_sent.append(tok)\n",
        "    return new_sent\n",
        "  \n",
        "def main():\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "    model = load_model('bert-large-uncased')\n",
        "\n",
        "    toks, seg_tensor, mask_ids = get_seed_sent(\"[CLS] Sing with me, Sing for the years [SEP] [MASK] [MASK] [MASK] [MASK] , [MASK] [MASK] [MASK] tears. [SEP]\",\n",
        "                                               tokenizer,\n",
        "                                               masking=None,\n",
        "                                               n_append_mask=0)\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        seg_tensor = seg_tensor.to('cuda')\n",
        "        model = model.to('cuda')\n",
        "        \n",
        "#     pred_toks = sequential_decoding(toks, seg_tensor, model, tokenizer, \"argmax\")\n",
        "    pred_toks = masked_decoding(toks, seg_tensor, mask_ids, model, tokenizer, \"argmax\")\n",
        "    \n",
        "    pred_lyric = detokenize(pred_toks)\n",
        "    \n",
        "    print(\"\\nFinal: %s\" % (\" \".join(pred_lyric)),\"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Final: [CLS] sing with me , sing for the years [SEP] sing for the years , cry no more tears . [SEP] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ErYYr10CkeMc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3005f0dd-9d56-4537-a54a-d39a320530c8"
      },
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-large-uncased')\n",
        "model.eval()\n",
        "\n",
        "sentence = '[CLS] Charles is a tailor [SEP] He is tall [SEP]'\n",
        "# sentence = '[CLS] Charles is a tailor [SEP] Excavation is important [SEP]'\n",
        "toks = tokenizer.tokenize(sentence)\n",
        "tok_ids = tokenizer.convert_tokens_to_ids(toks)\n",
        "tok_tensor = torch.LongTensor([tok_ids])\n",
        "token_type_ids_tensor = torch.LongTensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1]])\n",
        "seq_relationship_logits = model(tok_tensor, token_type_ids_tensor)\n",
        "print(seq_relationship_logits)\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 6.8790, -5.0848]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yFE4Y9s9u0iV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pytorch_pretrained_bert import BertTokenizer, BertForNextSentencePrediction\n",
        "\n",
        "def predict_next_sentence(sentenceA, sentenceBs, tokenizer, model):\n",
        "    sentenceA_toks = tokenizer.tokenize(sentenceA)\n",
        "    sentenceA_ids = tokenizer.convert_tokens_to_ids(sentenceA_toks)\n",
        "    sentenceA_types = [0] * len(sentenceA_ids)\n",
        "    sentenceA_attention = [1] * len(sentenceA_ids)\n",
        "    tok_ids = []\n",
        "    tok_types = []\n",
        "    tok_attention = []\n",
        "    \n",
        "    sentenceBs_ids = []\n",
        "    for sentenceB in sentenceBs:\n",
        "        sentenceB_toks = tokenizer.tokenize(sentenceB)\n",
        "        sentenceB_ids = tokenizer.convert_tokens_to_ids(sentenceB_toks)\n",
        "        sentenceBs_ids.append(sentenceB_ids)\n",
        "        \n",
        "    max_sentenceB_length = max(len(sentenceB_ids) for sentenceB_ids in sentenceBs_ids)\n",
        "    for sentenceB_ids in sentenceBs_ids:\n",
        "        padding_size = max_sentenceB_length - len(sentenceB_ids)\n",
        "        padded_sentenceB_ids = sentenceB_ids + [0] * padding_size\n",
        "        padded_sentenceB_types = [1] * max_sentenceB_length\n",
        "        padded_sentenceB_attention = [1] * len(sentenceB_ids) + [0] * padding_size\n",
        "        tok_ids.append(sentenceA_ids + padded_sentenceB_ids)\n",
        "        tok_types.append(sentenceA_types + padded_sentenceB_types)\n",
        "        tok_attention.append(sentenceA_attention + padded_sentenceB_attention)\n",
        "    tok_ids_tensor = torch.LongTensor(tok_ids)\n",
        "    tok_types_tensor = torch.LongTensor(tok_types)\n",
        "    tok_attention_tensor = torch.LongTensor(tok_attention)\n",
        "    seq_relationship_logits = model(tok_ids_tensor, tok_types_tensor, tok_attention_tensor)\n",
        "    return sentenceBs[seq_relationship_logits[:,0].argmax().tolist()]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UnGg7kPLvvr9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-large-uncased')\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8iwXmMgGvjK8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predicted_sentence = predict_next_sentence('[CLS] Charles is a tailor [SEP]', ['He is green [SEP]', 'He is very tall [SEP]', 'Excavation is important [SEP]'], tokenizer, model)\n",
        "print(predicted_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yGA5uTQ6p6xo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "import argparse\n",
        "\n",
        "def generate_predictions(args):\n",
        "    all_lines = []\n",
        "    all_pairs = []\n",
        "    with open(args.datafile, encoding='utf8') as csv_file:\n",
        "        csv_reader = csv.DictReader(csv_file)\n",
        "        for row in csv_reader:\n",
        "            lines = row['lyrics'].split('\\n')\n",
        "            for i in range(len(lines) - 1):\n",
        "                all_pairs.append((lines[i], lines[i + 1]))\n",
        "                all_lines.append(lines[i])\n",
        "            all_lines.append(lines[len(lines) - 1])\n",
        "\n",
        "    sampled_data_x = {}\n",
        "    sampled_data_y = {}\n",
        "    correct_pairs = random.sample(all_pairs, 100)\n",
        "    for line1, line2 in correct_pairs:\n",
        "        sampled_data_y[line1] = line2\n",
        "        sampled_data_x[line1] = [line2]\n",
        "        sampled_data_x[line1].extend(random.sample(all_lines, 2))\n",
        "\n",
        "    with open('predfile', 'w') as file:\n",
        "        for i, (line1, line2s) in enumerate(sampled_data_x.items()):\n",
        "#             line2 = random.choice(line2s)\n",
        "            line2 = predict_next_sentence(line1, line2s, tokenizer, model)\n",
        "            file.write(f'{line1}\\t{line2}\\n')\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f'Finished predicting {i + 1} lines...')\n",
        "    with open('goldfile', 'w') as file:\n",
        "        for line1, line2 in sampled_data_y.items():\n",
        "            file.write(f'{line1}\\t{line2}\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P1KhqEbuuTxD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "1cc54163-aed0-4125-9426-94132d6865cf"
      },
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--datafile', type=str, required=True)\n",
        "args = parser.parse_args(['--datafile', 'train_rock.csv'])\n",
        "generate_predictions(args)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished predicting 10 lines...\n",
            "Finished predicting 20 lines...\n",
            "Finished predicting 30 lines...\n",
            "Finished predicting 40 lines...\n",
            "Finished predicting 50 lines...\n",
            "Finished predicting 60 lines...\n",
            "Finished predicting 70 lines...\n",
            "Finished predicting 80 lines...\n",
            "Finished predicting 90 lines...\n",
            "Finished predicting 100 lines...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zc6gw-GC1RK-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}