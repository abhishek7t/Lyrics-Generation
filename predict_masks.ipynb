{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict_masks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "KYHRcp62BLqS",
        "colab_type": "code",
        "outputId": "a9c5e6de-1bfa-4d88-a07e-a6c8b6b6ce2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.0.1.post2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2018.1.10)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.9.131)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.16.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.18.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.2.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.131 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.12.131)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.3.9)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.131->boto3->pytorch_pretrained_bert) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.131->boto3->pytorch_pretrained_bert) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.131->boto3->pytorch_pretrained_bert) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Ih854I-CJcz",
        "colab_type": "code",
        "outputId": "1415fc32-0e37-4618-a3f6-7da6658ffdda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Try to predict a single masked-out word\"\"\"\n",
        "\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "\n",
        "# Tokenized input\n",
        "text1 = \"[CLS] And the riot squad they're restless, they need somewhere to go [SEP]\"\n",
        "text2 = \"As Lady and I look out tonight, from Desolation Row. [SEP]\"\n",
        "tokenized_text1 = tokenizer.tokenize(text1)\n",
        "tokenized_text2 = tokenizer.tokenize(text2)\n",
        "tokenized_text = tokenized_text1 + tokenized_text2\n",
        "\n",
        "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
        "masked_index = tokenized_text2.index('tonight') + len(tokenized_text1)\n",
        "tokenized_text[masked_index] = '[MASK]'\n",
        "\n",
        "# Convert token to vocabulary indices\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "segments_ids = [0]*len(tokenized_text1) + [1]*(len(tokenized_text2))\n",
        "\n",
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "if torch.cuda.is_available():\n",
        "    tokens_tensor = tokens_tensor.to('cuda')\n",
        "    segments_tensors = segments_tensors.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    predictions = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "# Confirm we were able to predict the correct '[MASK]'\n",
        "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
        "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "print(\"\\n\",\"Predicted [MASK] = \",predicted_token,\"\\n\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Predicted [MASK] =  together \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ja_iSf1TKhKK",
        "colab_type": "code",
        "outputId": "60a3da68-8ac7-4ac8-ff9a-7f1f2f7e702d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Try to generate from BERT \"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "MASK = \"[MASK]\"\n",
        "MASK_ATOM = \"[MASK]\"\n",
        "\n",
        "def preprocess(tokens, tokenizer):\n",
        "    \"\"\" Preprocess the sentence by tokenizing and converting to tensor \"\"\"\n",
        "    tok_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    tok_tensor = torch.tensor([tok_ids])\n",
        "    return tok_tensor\n",
        "\n",
        "\n",
        "def get_mask_ids(masking):\n",
        "    if masking:\n",
        "      mask_ids = [int(d) for d in masking.split(',')]\n",
        "    else:\n",
        "      mask_ids = []     \n",
        "    return mask_ids\n",
        "\n",
        "  \n",
        "def get_seed_sent(seed_sentence, tokenizer, masking=None, n_append_mask=0):\n",
        "    \"\"\" Get initial sentence to decode from, possible with masks \"\"\"\n",
        "\n",
        "    # Get initial mask\n",
        "    mask_ids = get_mask_ids(masking)\n",
        "\n",
        "    # Tokenize, respecting [MASK]\n",
        "    seed_sentence = seed_sentence.replace(MASK, MASK_ATOM)\n",
        "    toks = tokenizer.tokenize(seed_sentence)\n",
        "    for i, tok in enumerate(toks):\n",
        "        if tok == MASK_ATOM:\n",
        "            mask_ids.append(i)\n",
        "\n",
        "    # Mask the input\n",
        "    for mask_id in mask_ids:\n",
        "        toks[mask_id] = MASK\n",
        "\n",
        "    # Append MASKs\n",
        "    for _ in range(n_append_mask):\n",
        "        mask_ids.append(len(toks))\n",
        "        toks.append(MASK)\n",
        "    mask_ids = sorted(list(set(mask_ids)))\n",
        "\n",
        "    seg = [0] * len(toks)\n",
        "    seg_tensor = torch.tensor([seg])\n",
        "    return toks, seg_tensor, mask_ids\n",
        "\n",
        "  \n",
        "def load_model(version):\n",
        "    \"\"\" Load model \"\"\"\n",
        "    model = BertForMaskedLM.from_pretrained(version)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict(model, tokenizer, tok_tensor, seg_tensor, how_select=\"argmax\"):\n",
        "    \"\"\" Get model predictions and convert back to tokens \"\"\"\n",
        "    preds = model(tok_tensor, seg_tensor)\n",
        "\n",
        "    if how_select == \"sample\":\n",
        "        dist = Categorical(logits=F.log_softmax(preds[0], dim=-1))\n",
        "        pred_idxs = dist.sample().tolist()\n",
        "    elif how_select == \"topk\":\n",
        "        kth_vals, kth_idx = F.log_softmax(preds[0], dim=-1).topk(3, dim=-1)\n",
        "        dist = Categorical(logits=kth_vals)\n",
        "        pred_idxs = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1).tolist()\n",
        "    elif how_select == \"argmax\":\n",
        "        pred_idxs = preds.argmax(dim=-1).tolist()[0]\n",
        "    else:\n",
        "        raise NotImplementedError(\"Prediction procedure %s not found!\" % how_select)\n",
        "\n",
        "    pred_toks = tokenizer.convert_ids_to_tokens(pred_idxs)\n",
        "    return pred_toks\n",
        "\n",
        "def sequential_decoding(toks, seg_tensor, model, tokenizer, selection_strategy):\n",
        "    \"\"\" Decode from model one token at a time \"\"\"\n",
        "    for step_n in range(len(toks)):\n",
        "        tok_tensor = preprocess(toks, tokenizer)\n",
        "        pred_toks = predict(model, tokenizer, tok_tensor, seg_tensor, selection_strategy)\n",
        "        toks[step_n] = pred_toks[step_n]\n",
        "    return toks\n",
        "\n",
        "def masked_decoding(toks, seg_tensor, masks, model, tokenizer, selection_strategy):\n",
        "    \"\"\" Decode from model by replacing masks \"\"\"\n",
        "    for step_n, mask_id in enumerate(masks):\n",
        "        tok_tensor = preprocess(toks, tokenizer)\n",
        "        pred_toks = predict(model, tokenizer, tok_tensor, seg_tensor, selection_strategy)\n",
        "        toks[mask_id] = pred_toks[mask_id]\n",
        "    return toks\n",
        "\n",
        "  \n",
        "def detokenize(pred_toks):\n",
        "    \"\"\" Return the detokenized lyric prediction \"\"\"\n",
        "    new_sent = []\n",
        "    for i, tok in enumerate(pred_toks):\n",
        "        if tok.startswith(\"##\"):\n",
        "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
        "        else:\n",
        "            new_sent.append(tok)\n",
        "    return new_sent\n",
        "  \n",
        "def main():\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "    model = load_model('bert-large-uncased')\n",
        "\n",
        "    toks, seg_tensor, mask_ids = get_seed_sent(\"[CLS] Sing with me, Sing for the years [SEP] [MASK] [MASK] [MASK] [MASK] , [MASK] [MASK] [MASK] tears. [SEP]\",\n",
        "                                               tokenizer,\n",
        "                                               masking=None,\n",
        "                                               n_append_mask=0)\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        seg_tensor = seg_tensor.to('cuda')\n",
        "        model = model.to('cuda')\n",
        "        \n",
        "#     pred_toks = sequential_decoding(toks, seg_tensor, model, tokenizer, \"argmax\")\n",
        "    pred_toks = masked_decoding(toks, seg_tensor, mask_ids, model, tokenizer, \"argmax\")\n",
        "    \n",
        "    pred_lyric = detokenize(pred_toks)\n",
        "    \n",
        "    print(\"\\nFinal: %s\" % (\" \".join(pred_lyric)),\"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Final: [CLS] sing with me , sing for the years [SEP] sing for the years , cry no more tears . [SEP] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}